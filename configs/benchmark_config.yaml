# EPD Benchmark Configuration
# This file configures both vLLM EPD and ElasticMM for fair comparison

# Model configuration
model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  # Alternative models:
  # name: "Qwen/Qwen2.5-VL-7B-Instruct"
  # name: "llava-hf/llava-v1.6-mistral-7b-hf"

# Dataset configuration
dataset:
  # Image directory for multimodal requests
  image_dir: "/path/to/images"
  # Text data path (JSONL format with conversations)
  text_data_path: "/path/to/conversations.jsonl"
  # HuggingFace dataset for vLLM bench
  hf_dataset: "lmarena-ai/VisionArena-Chat"

# Workload configuration
workload:
  # Duration of the benchmark in seconds
  duration_seconds: 600
  # Number of prompts for vLLM bench serve
  num_prompts: 100

  # Request rates (requests per second)
  text_base_rate: 30.0
  multimodal_base_rate: 20.0

  # Variance in request rates (for dynamic workload)
  text_variance: 0.3
  multimodal_variance: 1.5

  # Request configuration
  max_tokens: 512
  temperature: 0.7

# GPU configuration
gpus:
  # Total available GPUs
  total: 4

  # vLLM EPD configuration (1E1P1D requires 3 GPUs)
  vllm:
    encoder: 0
    prefill: 1
    decode: 2

  # ElasticMM configuration
  elasticmm:
    text_gpus: 1
    multimodal_gpus: 3

# Server ports
ports:
  vllm:
    encoder: 19534
    prefill: 19535
    decode: 19536
    proxy: 10001
  elasticmm:
    proxy: 10002
    service_discovery: 30002

# Shared storage paths
storage:
  ec_cache: "/tmp/ec_cache"
  logs: "./logs"
  results: "./results"

# Benchmark settings
benchmark:
  # Warmup period before collecting metrics (seconds)
  warmup_seconds: 30
  # Number of runs for statistical significance
  num_runs: 3
  # Metrics collection interval (seconds)
  metrics_interval: 1.0
